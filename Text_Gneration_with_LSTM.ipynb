{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJkCvL-iWlp5"
      },
      "source": [
        "# Importing libraies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDhe1uLZuZl2",
        "outputId": "2193ff9c-70d8-4329-a6d0-ae8bebe90a43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Activation, Dropout, Embedding, Input, Bidirectional, GlobalMaxPooling1D, Attention, BatchNormalization, Flatten\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from numba import jit, cuda\n",
        "import re\n",
        "from string import punctuation\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import keras.utils as ku\n",
        "from keras.callbacks import EarlyStopping\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kun2ShYEuex-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def remove_four_spaces(line):\n",
        "    new_line = \"\"\n",
        "    for idx, char in enumerate(line):\n",
        "        if char == ' ' and idx + 3 < len(line) and line[idx + 1] == ' ' and line[idx + 2] == ' ':\n",
        "            new_line += line[idx + 4:]\n",
        "            break\n",
        "    return new_line\n",
        "\n",
        "def preprocess_text(text):\n",
        "    patterns = [('+', ''), ('$', ''), ('murderland', ''), ('\\d+', '')]\n",
        "    for old, new in patterns:\n",
        "        text = text.replace(old, new)\n",
        "    lines = text.splitlines()\n",
        "    new_text_lines = []\n",
        "    for line in lines:\n",
        "        new_line = remove_four_spaces(line)\n",
        "        new_text_lines.append(new_line)\n",
        "    new_text = \"\\n\".join(new_text_lines)\n",
        "    new_text = new_text.replace('\\n\\n', \"\")\n",
        "    new_text = new_text.translate(str.maketrans(\"\", \"\", punctuation))\n",
        "    return new_text.lower()\n",
        "\n",
        "def load_script(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        script = f.read()\n",
        "    return script\n",
        "\n",
        "def remove_empty_lines(text):\n",
        "    lines = text.split(\"\\n\")\n",
        "    cleaned_lines = [line.strip() for line in lines if line.strip()]\n",
        "    cleaned_text = \"\\n\".join(cleaned_lines)\n",
        "    return cleaned_text\n",
        "\n",
        "def remove_numbers(text):\n",
        "    cleaned_text = re.sub(r'\\d+', '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "def remove_uppercase_with_spaces_or_adjacent_and_spaces_before_first_word(text):\n",
        "    pattern = r'(?:\\s{3,})([A-Z]+)(?:\\s{3,})|([A-Z]+)(?:\\s{3,})|(?:\\s{3,})([A-Z]+\\s[A-Z]+)'\n",
        "    cleaned_text = re.sub(pattern, '', text)\n",
        "    return cleaned_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRHGYqsquhnN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load scripts\n",
        "#movie_quotes_script = load_script(\"/content/drive/MyDrive/moviequotes.scripts.txt\")\n",
        "pulp_fiction_script = load_script(\"/content/drive/MyDrive/Pulp Fiction script.txt\")\n",
        "Stephen_King_script = load_script(\"/content/drive/MyDrive/Stephen King Script.log\")\n",
        "\n",
        "filtered_text = \"\"\n",
        "with open(\"/content/drive/MyDrive/jocker_script.txt\", \"r\") as file:\n",
        "    # Read each line in the file\n",
        "    for line_num, line in enumerate(file, start=1):\n",
        "        # Check if the line contains any lowercase letters\n",
        "        if any(char.islower() for char in line):\n",
        "            filtered_text += line\n",
        "\n",
        "cleaned_text = re.sub(r'\\(.*?\\)', '', filtered_text, flags=re.DOTALL)\n",
        "cleaned_text = '\\n'.join(line.lstrip() for line in cleaned_text.split('\\n'))\n",
        "word_to_delete = \"Beat.\"\n",
        "cleaned_text = '\\n'.join(line for line in cleaned_text.split('\\n') if line.strip() != word_to_delete)\n",
        "cleaned_text = cleaned_text.translate(str.maketrans(\"\", \"\", punctuation))\n",
        "jocker_script = cleaned_text.replace(\"’\", \"\").lower()\n",
        "jocker_script = remove_empty_lines(jocker_script)\n",
        "jocker_script = remove_numbers(jocker_script)\n",
        "\n",
        "# Preprocess scripts\n",
        "#movie_quotes_script = preprocess_text(movie_quotes_script)\n",
        "#print(jocker_script)\n",
        "jocker_len = len(jocker_script.split())\n",
        "jocker_lenLine = len(jocker_script.splitlines())\n",
        "\n",
        "filtered_text = \"\"\n",
        "with open(\"/content/drive/MyDrive/THE SHAWSHANK REDEMPTION.txt\", \"r\") as file:\n",
        "    # Read each line in the file\n",
        "    for line_num, line in enumerate(file, start=1):\n",
        "        # Check if the line contains any lowercase letters\n",
        "        if any(char.islower() for char in line):\n",
        "            filtered_text += line\n",
        "\n",
        "THE_SHAWSHANK_script = filtered_text.translate(str.maketrans(\"\", \"\", punctuation))\n",
        "THE_SHAWSHANK_script = THE_SHAWSHANK_script.replace(\"’\", \"\").lower()\n",
        "\n",
        "for lineS in THE_SHAWSHANK_script.splitlines():\n",
        "  if 'int' in lineS:\n",
        "    THE_SHAWSHANK_script = THE_SHAWSHANK_script.replace(lineS, \"\")\n",
        "\n",
        "THE_SHAWSHANK_script = remove_empty_lines(THE_SHAWSHANK_script)\n",
        "THE_SHAWSHANK_script = remove_numbers(THE_SHAWSHANK_script)\n",
        "THE_SHAWSHANK_len = len(THE_SHAWSHANK_script.split())\n",
        "THE_SHAWSHANK_lenLine = len(THE_SHAWSHANK_script.splitlines())\n",
        "\n",
        "\n",
        "\n",
        "pulp_fiction_script = remove_uppercase_with_spaces_or_adjacent_and_spaces_before_first_word(pulp_fiction_script)\n",
        "pulp_fiction_script = pulp_fiction_script.lower()\n",
        "pulp_fiction_script = pulp_fiction_script.translate(str.maketrans(\"\", \"\", punctuation))\n",
        "pulp_fiction_script = pulp_fiction_script.replace(\"’\", \"\")\n",
        "pulp_fiction_script = remove_empty_lines(pulp_fiction_script)\n",
        "pulp_fiction_script = remove_numbers(pulp_fiction_script)\n",
        "\n",
        "pulp_len = len(pulp_fiction_script.split())\n",
        "pulp_lenLine = len(pulp_fiction_script.splitlines())\n",
        "\n",
        "\n",
        "Stephen_King_script = Stephen_King_script.lower()\n",
        "Stephen_King_script = Stephen_King_script.translate(str.maketrans(\"\", \"\", punctuation))\n",
        "Stephen_King_script = Stephen_King_script.replace(\"’\", \"\")\n",
        "Stephen_King_script = remove_empty_lines(Stephen_King_script)\n",
        "Stephen_King_script = remove_numbers(Stephen_King_script)\n",
        "\n",
        "\n",
        "Stephen_King_len = len(Stephen_King_script.split())\n",
        "Stephen_King_lenLine = len(Stephen_King_script.splitlines())\n",
        "\n",
        "#print(Stephen_King_script)\n",
        "\n",
        "text = jocker_script +'\\n'  + THE_SHAWSHANK_script + '\\n' + pulp_fiction_script + '\\n' + Stephen_King_script\n",
        "# Create tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "corpus = text.lower().split(\"\\n\")\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzWCtyXFuqPe"
      },
      "outputs": [],
      "source": [
        "# Create tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "corpus = text.lower().split(\"\\n\")\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "# Create input sequences\n",
        "input_seq = []\n",
        "lenth = 0\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_seq = token_list[:i + 1]\n",
        "        input_seq.append(n_gram_seq)\n",
        "\n",
        "\n",
        "max_seq_len = max([len(x) for x in input_seq])\n",
        "input_seq = np.array(pad_sequences(input_seq, maxlen = max_seq_len, padding='pre'))\n",
        "predictors, label = input_seq[:, :-1], input_seq[:, -1]\n",
        "\n",
        "label = ku.to_categorical(label, num_classes=total_words)\n",
        "\n",
        "# Load GloVe embeddings\n",
        "# GloVe dive link : https://drive.google.com/file/d/1IhO9d0ZzQjAlZKzsYZLAzJ4-taC_ljAb/view?usp=drive_link\n",
        "glove_path = '/content/drive/MyDrive/glove.6B.100d.txt'\n",
        "embeddings_index = dict()\n",
        "with open(glove_path, encoding=\"utf8\") as glove:\n",
        "    for line in glove:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "glove.close()\n",
        "\n",
        "embedding_matrix = np.zeros((total_words, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index > total_words - 1:\n",
        "        break\n",
        "    else:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG4qXiQ6SRkg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3277cc4-56cf-42c0-bb2f-5f5e78ece17f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-0.038194  , -0.24487001,  0.72812003, ..., -0.1459    ,\n",
              "         0.82779998,  0.27061999],\n",
              "       [-0.27085999,  0.044006  , -0.02026   , ..., -0.4923    ,\n",
              "         0.63687003,  0.23642001],\n",
              "       ...,\n",
              "       [-0.0061811 ,  0.25497001,  0.52074999, ..., -0.28420001,\n",
              "        -0.70090002,  0.047897  ],\n",
              "       [ 0.47593001,  0.35716   , -0.24327999, ..., -0.097822  ,\n",
              "         0.9131    , -0.18909   ],\n",
              "       [-0.02838   ,  0.47933999, -0.47825   , ..., -0.45166999,\n",
              "         0.10207   ,  0.099218  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73YmHr56yy0q"
      },
      "outputs": [],
      "source": [
        "first_trainL = label[:jocker_len - jocker_lenLine]\n",
        "second_trainL = label[jocker_len - jocker_lenLine : THE_SHAWSHANK_len + jocker_len - jocker_lenLine - THE_SHAWSHANK_lenLine]\n",
        "third_trainL = label[THE_SHAWSHANK_len + jocker_len - jocker_lenLine - THE_SHAWSHANK_lenLine : pulp_len + THE_SHAWSHANK_len + jocker_len - jocker_lenLine - THE_SHAWSHANK_lenLine - pulp_lenLine]\n",
        "fourth_trainL = label[pulp_len + THE_SHAWSHANK_len + jocker_len - jocker_lenLine - THE_SHAWSHANK_lenLine - pulp_lenLine : ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFJRbsyoG2TU"
      },
      "outputs": [],
      "source": [
        "first_trainP = predictors[:jocker_len - jocker_lenLine]\n",
        "second_trainP = predictors[jocker_len - jocker_lenLine : THE_SHAWSHANK_len + jocker_len - jocker_lenLine - THE_SHAWSHANK_lenLine]\n",
        "third_trainP = predictors[THE_SHAWSHANK_len + jocker_len - jocker_lenLine - THE_SHAWSHANK_lenLine : pulp_len + THE_SHAWSHANK_len + jocker_len - jocker_lenLine - THE_SHAWSHANK_lenLine - pulp_lenLine]\n",
        "fourth_trainP = predictors[pulp_len + THE_SHAWSHANK_len + jocker_len - jocker_lenLine - THE_SHAWSHANK_lenLine - pulp_lenLine : ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDNTn6sEo4Lu"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        et = K.squeeze(K.tanh(K.dot(x, self.W) + self.b), axis=-1)\n",
        "        at = K.softmax(et)\n",
        "        at = K.expand_dims(at, axis=-1)\n",
        "        output = x * at\n",
        "        return K.sum(output, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B69FIdhru7Ce"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(total_words, 100, weights=[embedding_matrix], input_length=max_seq_len - 1))\n",
        "model2.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "model2.add(AttentionLayer())\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(total_words, activation='softmax'))\n",
        "model2.summary()\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model2.fit(predictors, label, validation_split=0.2, epochs=20, batch_size=128, verbose=1)\n",
        "model2.save('/content/drive/MyDrive/TextModel2.h5')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogf2n4JTCSoX"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model, Model\n",
        "from keras.utils import CustomObjectScope\n",
        "\n",
        "# Define a function to load the model with custom objects\n",
        "def load_model_with_custom_objects(model_path):\n",
        "    with CustomObjectScope({'AttentionLayer': AttentionLayer}):\n",
        "        return load_model(model_path)\n",
        "\n",
        "# model on dive : https://drive.google.com/file/d/1-21TX3i-TKUr6MmpupCTp-61NTm0vGoR/view?usp=drive_link\n",
        "model2 = load_model_with_custom_objects('/content/drive/MyDrive/TextModel2.h5')\n",
        "model2.summary()\n",
        "# Assuming 'model2' is your pre-trained model\n",
        "\n",
        "# Assuming `history` is the object returned by `model.fit()`\n",
        "model2.fit(predictors, label, validation_split=0.2, epochs=3, batch_size=128, verbose=1)\n",
        "model2.save('/content/drive/MyDrive/TextModel2.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCbcSjvbpqBL",
        "outputId": "4341e1ea-a187-4729-c43c-be75f58bfd40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 21, 100)           1019000   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 21, 512)           731136    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " attention_layer (Attention  (None, 512)               533       \n",
            " Layer)                                                          \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10190)             5227470   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6978139 (26.62 MB)\n",
            "Trainable params: 6978139 (26.62 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "\n",
            "Generation :i like to walk away youre going to lose what i am to take a man to answer from the grease spot in the\n"
          ]
        }
      ],
      "source": [
        "def generate_text(seed_text, next_words, max_seq_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_seq_len - 1, padding='pre')\n",
        "        predicted = model2.predict(token_list, verbose=0)\n",
        "        index = np.argmax(predicted[0])\n",
        "        output_word = \"\"\n",
        "        for word, idx in tokenizer.word_index.items():\n",
        "            if idx == index:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "# Train the model\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model, Model\n",
        "from keras.utils import CustomObjectScope\n",
        "\n",
        "# Define a function to load the model with custom objects\n",
        "def load_model_with_custom_objects(model_path):\n",
        "    with CustomObjectScope({'AttentionLayer': AttentionLayer}):\n",
        "        return load_model(model_path)\n",
        "\n",
        "\n",
        "model2 = load_model_with_custom_objects('/content/drive/MyDrive/TextModel2.h5')\n",
        "model2.summary()\n",
        "\n",
        "\n",
        "# Example reference and generated text\n",
        "reference_text = \" \".join(Stephen_King_script.split()[:25])\n",
        "generated_text = generate_text(\" \".join(Stephen_King_script.split()[:5]), 20, max_seq_len)\n",
        "\n",
        "#print(f'\\nGeneration :{generated_text}')\n",
        "#print(f'Refrence: {reference_text}\\n')\n",
        "\n",
        "#print('='*200)\n",
        "\n",
        "\n",
        "generated_text2 = generate_text('i like to walk', 20, max_seq_len)\n",
        "print(f'\\nGeneration :{generated_text2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojqtjmFvYGhU"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu\n",
        "\n",
        "def bleu(refs, gens):\n",
        "    '''\n",
        "    Calculate pair-wise BLEU score using NLTK implementation.\n",
        "\n",
        "    Args:\n",
        "        refs : a list of reference sentences\n",
        "        gens : a list of candidate (generated) sentences\n",
        "\n",
        "    Returns:\n",
        "        bleu score (float)\n",
        "    '''\n",
        "    ref_bleu = []\n",
        "    gen_bleu = []\n",
        "\n",
        "    for ref, gen in zip(refs, gens):\n",
        "        ref_bleu.append([ref.split()])\n",
        "        gen_bleu.append(gen.split())\n",
        "\n",
        "    cc = SmoothingFunction()\n",
        "    score_bleu = corpus_bleu(ref_bleu, gen_bleu, weights=(0, 1, 0, 0), smoothing_function=cc.method4)\n",
        "    return score_bleu\n",
        "\n",
        "import itertools\n",
        "\n",
        "# Supporting function\n",
        "def _split_into_words(sentences):\n",
        "    \"\"\"Splits multiple sentences into words and flattens the result\"\"\"\n",
        "    return list(itertools.chain(*[_.split(\" \") for _ in sentences]))\n",
        "\n",
        "# Supporting function\n",
        "def _get_word_ngrams(n, sentences):\n",
        "    \"\"\"Calculates word n-grams for multiple sentences.\"\"\"\n",
        "    assert len(sentences) > 0\n",
        "    assert n > 0\n",
        "\n",
        "    words = _split_into_words(sentences)\n",
        "    return _get_ngrams(n, words)\n",
        "\n",
        "# Supporting function\n",
        "def _get_ngrams(n, text):\n",
        "    \"\"\"Calculates n-grams.\"\"\"\n",
        "    ngram_set = set()\n",
        "    text_length = len(text)\n",
        "    max_index_ngram_start = text_length - n\n",
        "    for i in range(max_index_ngram_start + 1):\n",
        "        ngram_set.add(tuple(text[i:i + n]))\n",
        "    return ngram_set\n",
        "\n",
        "def rouge_n(reference_sentences, evaluated_sentences, n=2):\n",
        "    \"\"\"\n",
        "    Computes ROUGE-N of two text collections of sentences.\n",
        "    Args:\n",
        "        evaluated_sentences: The sentences that have been picked by the summarizer.\n",
        "        reference_sentences: The sentences from the reference set.\n",
        "        n: Size of n-grams. Defaults to 2.\n",
        "    Returns:\n",
        "        Recall ROUGE score (float).\n",
        "    Raises:\n",
        "        ValueError: Raises exception if a param has len <= 0.\n",
        "    \"\"\"\n",
        "    if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
        "        raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
        "\n",
        "    evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)\n",
        "    reference_ngrams = _get_word_ngrams(n, reference_sentences)\n",
        "    reference_count = len(reference_ngrams)\n",
        "    evaluated_count = len(evaluated_ngrams)\n",
        "\n",
        "    # Gets the overlapping n-grams between evaluated and reference\n",
        "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
        "    overlapping_count = len(overlapping_ngrams)\n",
        "\n",
        "    # Handle edge case. This isn't mathematically correct, but it's good enough\n",
        "    if evaluated_count == 0:\n",
        "        precision = 0.0\n",
        "    else:\n",
        "        precision = overlapping_count / evaluated_count\n",
        "\n",
        "    if reference_count == 0:\n",
        "        recall = 0.0\n",
        "    else:\n",
        "        recall = overlapping_count / reference_count\n",
        "\n",
        "    # F1-score is not needed, just returning recall count in ROUGE, useful for our purpose\n",
        "    return recall\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_Y6V29eX8g-",
        "outputId": "247567e3-1d2d-4866-dd61-2e42d42febdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score: 0.0034871578252686666\n",
            "ROUGE Recall Score 1: 0.4117647058823529\n",
            "ROUGE Recall Score 2: 0.2631578947368421\n"
          ]
        }
      ],
      "source": [
        "bleu_score = bleu(reference_text, generated_text)\n",
        "recall_score1= rouge_n([reference_text], [generated_text], 1)\n",
        "recall_score2 = rouge_n([reference_text], [generated_text], 2)\n",
        "\n",
        "print(\"BLEU Score:\", bleu_score)\n",
        "print(\"ROUGE Recall Score 1:\", recall_score1)\n",
        "print(\"ROUGE Recall Score 2:\", recall_score2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiTaX7RPPwvZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}